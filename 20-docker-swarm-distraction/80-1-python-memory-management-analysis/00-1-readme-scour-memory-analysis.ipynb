{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Ananlysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://medium.com/survata-engineering-blog/monitoring-memory-usage-of-a-running-python-program-49f027e3d1ba\n",
    "\n",
    "Memory Allocation and Management in Python - simplified tutorial for beginners\n",
    "https://www.youtube.com/watch?v=arxWaw-E8QQ\n",
    "\n",
    "\n",
    "Python Tips & Tricks: How to Check Memory Usage\n",
    "https://www.youtube.com/watch?v=_OlNOKg3PpY\n",
    "\n",
    " simplefunde  - lots of ComSci Concepts\n",
    "https://www.youtube.com/channel/UC0ob0iy_DRTyL00IsgSMzgw\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ps\n",
    "\n",
    "```\n",
    "$ ps -m -o %cpu,%mem,command\n",
    "%CPU %MEM COMMAND\n",
    "23.4  7.2 python analyze_data.py\n",
    " 0.0  0.0 bash\n",
    "```\n",
    "\n",
    "**watch free -h** - # system wide if you are only running one app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profilers\n",
    "\n",
    "- like py-spy\n",
    "https://pypi.org/project/memory-profiler/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This containerization process obscures how much memory is being used inside the container. \n",
    "\n",
    "So — we need to monitor memory usage inside the container.\n",
    "\n",
    "Your first inclination might be to use the same operating system techniques, but inside the container. While this does technically work, general advice is that a Docker container should run a single process — so running a second monitoring process inside a container isn’t a good option.\n",
    "\n",
    "# tracemalloc\n",
    "\n",
    "## example\n",
    "\n",
    "```\n",
    "import tracemalloc\n",
    "\n",
    "tracemalloc.start()\n",
    "my_complex_analysis_method()\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "print(f\"Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")\n",
    "tracemalloc.stop()\n",
    "```\n",
    "\n",
    "There’s a price to be paid for this level of detail, though. tracemalloc injects itself deep into the running Python process — which, as you might expect, comes with a performance cost. In our testing, we observed a 30% slowdown when using tracemalloc on a running analysis run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 3: Sampling\n",
    "\n",
    "Luckily, the Python standard library provides another way to observe memory usage — the resource module. The resource module provides basic controls for resources that a program allocates — including memory usage:\n",
    "\n",
    "```\n",
    "import resource\n",
    "\n",
    "usage = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import resource\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "class MemoryMonitor:\n",
    "    def __init__(self):\n",
    "        self.keep_measuring = True\n",
    "\n",
    "    def measure_usage(self):\n",
    "        max_usage = 0\n",
    "        while self.keep_measuring:\n",
    "            max_usage = max(\n",
    "                max_usage,\n",
    "                resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
    "            )\n",
    "            sleep(0.1)\n",
    "\n",
    "        return max_usage\n",
    "```\n",
    "But what tells the loop to exit? And where do we call the code being monitored? We do that in a separate thread.\n",
    "\n",
    "```\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    monitor = MemoryMonitor()\n",
    "    mem_thread = executor.submit(monitor.memory_usage)\n",
    "    try:\n",
    "        fn_thread = executor.submit(my_analysis_function)\n",
    "        result = fn_thread.result()\n",
    "    finally:\n",
    "        monitor.keep_measuring = False\n",
    "        max_usage = mem_thread.result()\n",
    "        \n",
    "    print(f\"Peak memory usage: {max_usage}\")\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "It’s impossible to improve something you aren’t measuring. Armed with more information about the memory usage of our analysis tasks, we’re now in a much better position to optimize our resource usage. And, we’ve been able to collect that information with relatively little code and relatively little performance overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More References\n",
    "\n",
    "https://medium.com/the-andela-way/machine-monitoring-tool-using-python-from-scratch-8d10411782fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
